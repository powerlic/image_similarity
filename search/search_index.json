{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Auto-encoder based Image-Similarity Engine Builds a simple Convolutional Auto-encoder based Image similarity engine. Convolutional Auto-encoder Convolutional autoencoder consists of two parts, encoder and decoer. Encoder CNN Encoder converts the given input image into an embedding representation of size (bs, c, h, w) It contains of several CNN, RELU, MaxPool2d layers on top of each other. Decoder CNN Decoder converts the image generated from Encoder back to the input image. It consists of Conv2DTranspose layers, which is transposed convulation operation, helping to upsample the size. Alternatively, it can also be made using Bilinear interpolation or Upsampling layer to upsample to orignal image size. Auto-Encoder Auto-encoder combines both encoder and decoder to learn a feature representation of input images. Both the parameters are combined and trained with a single common loss function and optimizer. The encoder layers give us a resultant latent representation of images through convolutional layers.","title":"Convolutional Auto-encoder"},{"location":"#auto-encoder-based-image-similarity-engine","text":"Builds a simple Convolutional Auto-encoder based Image similarity engine.","title":"Auto-encoder based Image-Similarity Engine"},{"location":"#convolutional-auto-encoder","text":"Convolutional autoencoder consists of two parts, encoder and decoer. Encoder CNN Encoder converts the given input image into an embedding representation of size (bs, c, h, w) It contains of several CNN, RELU, MaxPool2d layers on top of each other. Decoder CNN Decoder converts the image generated from Encoder back to the input image. It consists of Conv2DTranspose layers, which is transposed convulation operation, helping to upsample the size. Alternatively, it can also be made using Bilinear interpolation or Upsampling layer to upsample to orignal image size.","title":"Convolutional Auto-encoder"},{"location":"#auto-encoder","text":"Auto-encoder combines both encoder and decoder to learn a feature representation of input images. Both the parameters are combined and trained with a single common loss function and optimizer. The encoder layers give us a resultant latent representation of images through convolutional layers.","title":"Auto-Encoder"},{"location":"creating_embeddings/","text":"Creating Embeddings. To find similar images, we first need to create embeddings from given images. To create embeddings we make use of the convolutional auto-encoder. This is an unsupervised problem where we use auto-encoders to reconstruct the image. In tihs porcess the encoder learns embeddings of given images while decoder helps to reconstruct. It can be summarized as follows encoded_image = encoder(input_image) reoncstructed_image = decoder(encoded_image) We calculate loss between reconstructed image and input image. loss_fn = nn.MSELoss() loss = loss_fn(input_image, reconstructed_image) loss.backward() optimizer.step() Optimizer updates both encoder and decoder. Storing Embeddings The output of convolutional encoder is collected across all the image batches into a single tensor. It's dimension depends on number of CNN encoding layers used which is the embedding dimension. E.g. For 4000 images if embedding dimension is (16, 256, 256) We get representation as (4000, 16, 256, 256) We flatten these embeddings into numpy arrays of dimension (number_of_images, c * h * w) Where c, h, w are channels, height and width of the images. We save these embeddings in numpy \"npy\" format.","title":"Creating Embeddings"},{"location":"creating_embeddings/#creating-embeddings","text":"To find similar images, we first need to create embeddings from given images. To create embeddings we make use of the convolutional auto-encoder. This is an unsupervised problem where we use auto-encoders to reconstruct the image. In tihs porcess the encoder learns embeddings of given images while decoder helps to reconstruct. It can be summarized as follows encoded_image = encoder(input_image) reoncstructed_image = decoder(encoded_image) We calculate loss between reconstructed image and input image. loss_fn = nn.MSELoss() loss = loss_fn(input_image, reconstructed_image) loss.backward() optimizer.step() Optimizer updates both encoder and decoder.","title":"Creating Embeddings."},{"location":"creating_embeddings/#storing-embeddings","text":"The output of convolutional encoder is collected across all the image batches into a single tensor. It's dimension depends on number of CNN encoding layers used which is the embedding dimension. E.g. For 4000 images if embedding dimension is (16, 256, 256) We get representation as (4000, 16, 256, 256) We flatten these embeddings into numpy arrays of dimension (number_of_images, c * h * w) Where c, h, w are channels, height and width of the images. We save these embeddings in numpy \"npy\" format.","title":"Storing Embeddings"},{"location":"image_cluster/","text":"Image Clustering Embeddings which are learnt from convolutional Auto-encoder are used to cluster the images. Since the dimensionality of Embeddings is big. We first reduce it by fast dimensionality reduction technique such as PCA. This is required as T-SNE is much slower and would take lot of time and memory in clustering huge embeddings. After that we use T-SNE (T-Stochastic Nearest Embedding) to reduce the dimensionality further. Since these are unsupervised embeddings. Clustering might help us to find classes. Clustering output. The clusters are note quite clear as model used in very simple one. T-SNE is takes time to converge and needs lot of tuning. Also the embeddings can be learnt much better with pretrained models, etc. The clustering script can be found here It can be used with any arbitrary 2 dimensional embedding learnt using Auto-Encoders.","title":"Clustering Images"},{"location":"image_cluster/#image-clustering","text":"Embeddings which are learnt from convolutional Auto-encoder are used to cluster the images. Since the dimensionality of Embeddings is big. We first reduce it by fast dimensionality reduction technique such as PCA. This is required as T-SNE is much slower and would take lot of time and memory in clustering huge embeddings. After that we use T-SNE (T-Stochastic Nearest Embedding) to reduce the dimensionality further. Since these are unsupervised embeddings. Clustering might help us to find classes.","title":"Image Clustering"},{"location":"image_cluster/#clustering-output","text":"The clusters are note quite clear as model used in very simple one. T-SNE is takes time to converge and needs lot of tuning. Also the embeddings can be learnt much better with pretrained models, etc. The clustering script can be found here It can be used with any arbitrary 2 dimensional embedding learnt using Auto-Encoders.","title":"Clustering output."},{"location":"image_features/","text":"Computing Images closest to given features. We determine the prominent features from a given input image. To determine the features which are required. It uses cv2.ORB() a fast technique for finding oriented, robust features. ORB (Oriented FAST and Rotated BRIEF) gives top features in the given input image. The features from ORB are converted to embedding. We reduce dimensionality of embeddings obtained from auto-encoder to ORB embeddings. This allows us to search for to matches using Nearest Neighbors. Then it searches through the embeddings obtained from auto-encoder for images similar to embeddings from ORB. Thus we get images that are similar to given features in an image. Outputs: - This is an image whose features have been computed using ORB. Recommended images, matching these features. I have plotted keypoints on them. We can see that the features extracted are similar.","title":"Finding Similar Features"},{"location":"image_features/#computing-images-closest-to-given-features","text":"We determine the prominent features from a given input image. To determine the features which are required. It uses cv2.ORB() a fast technique for finding oriented, robust features. ORB (Oriented FAST and Rotated BRIEF) gives top features in the given input image. The features from ORB are converted to embedding. We reduce dimensionality of embeddings obtained from auto-encoder to ORB embeddings. This allows us to search for to matches using Nearest Neighbors. Then it searches through the embeddings obtained from auto-encoder for images similar to embeddings from ORB. Thus we get images that are similar to given features in an image.","title":"Computing Images closest to given features."},{"location":"image_features/#outputs-","text":"This is an image whose features have been computed using ORB. Recommended images, matching these features. I have plotted keypoints on them. We can see that the features extracted are similar.","title":"Outputs: -"},{"location":"image_simi/","text":"Image similarity searching. We have to solve problem given in the below figure Given a new Query Image, we need to find most similar images from embeddings and return them. Here, we make use of embeddings that we have learnt. These embeddings contain representation of images on which the convolutional encoder was trained on. Creating Embedding for Query Image and Searching We convert the query Image to query_embedding using the encoder. This query_embedding is used to search for similar images from embedding which we learnt before using autoencoder. Now, we use K-Nearest Neighbors to find K number of nearest embeddings. These nearest embeddings are those in pre-learnt embedding which are nearest to query_embedding. Use the indices of these nearest embeddings, we display the most similar image from our images folder. Models Some models trained using a sample dataset are given in this link. It contains encoder, decoder and numpy embeddings generated from auto-enocoders. These are obtained by running torch_train.py script. Outputs Query Image Recommended Images","title":"Finding Similar Images"},{"location":"image_simi/#image-similarity-searching","text":"We have to solve problem given in the below figure Given a new Query Image, we need to find most similar images from embeddings and return them. Here, we make use of embeddings that we have learnt. These embeddings contain representation of images on which the convolutional encoder was trained on.","title":"Image similarity searching."},{"location":"image_simi/#creating-embedding-for-query-image-and-searching","text":"We convert the query Image to query_embedding using the encoder. This query_embedding is used to search for similar images from embedding which we learnt before using autoencoder. Now, we use K-Nearest Neighbors to find K number of nearest embeddings. These nearest embeddings are those in pre-learnt embedding which are nearest to query_embedding. Use the indices of these nearest embeddings, we display the most similar image from our images folder.","title":"Creating Embedding for Query Image and Searching"},{"location":"image_simi/#models","text":"Some models trained using a sample dataset are given in this link. It contains encoder, decoder and numpy embeddings generated from auto-enocoders. These are obtained by running torch_train.py script.","title":"Models"},{"location":"image_simi/#outputs","text":"Query Image Recommended Images","title":"Outputs"}]}